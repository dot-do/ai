---
$id: https://context.do
$type: https://schema.org/SoftwareSourceCode
name: context.do
description: AI context window management, workflow context passing, and semantic context patterns for the .do platform
version: 1.0.0
license: CC-BY-4.0
author:
  $type: https://schema.org/Organization
  name: .do
  url: https://do.inc
keywords:
  - context management
  - AI context window
  - token management
  - context compression
  - semantic context
  - workflow context
  - conversation history
  - context patterns
programmingLanguage: TypeScript
runtimePlatform: Node.js, Cloudflare Workers
---

# context.do

**AI Context Window Management, Workflow Context Passing, and Semantic Context Patterns**

`context.do` provides intelligent context management for AI applications, enabling efficient use of limited context windows, seamless workflow context passing, and semantic context patterns that preserve meaning while optimizing token usage.

## Overview

Modern AI applications face critical challenges with context management:

- **Limited Context Windows**: GPT-5 (128K tokens), Claude Sonnet 4.5 (200K tokens) have finite capacity
- **Token Costs**: Every token in context costs money and latency
- **Context Overflow**: Long conversations or large documents exceed limits
- **Semantic Preservation**: Compression must maintain meaning and relevance
- **Multi-Turn Conversations**: History accumulates quickly
- **Workflow Context**: Passing state between autonomous agents

`context.do` solves these challenges with intelligent context management that understands your application's semantic structure and optimizes context automatically.

## Key Features

### 1. Intelligent Context Compression

Automatically compress context while preserving semantic meaning:

```typescript
import { $ } from 'sdk.do'

// Create compressed context from long conversation
const context = await $.Context.compress({
  messages: conversationHistory,
  targetTokens: 4000,
  preserveRecent: 3, // Keep last 3 messages uncompressed
  strategy: 'semantic', // 'semantic' | 'sliding' | 'summarize' | 'hierarchical'
})

// Use compressed context in AI generation
const response = await $.ai.generate({
  model: 'gpt-5',
  context: context.compressed,
  prompt: 'Continue the conversation',
})
```

### 2. Token Budget Management

Monitor and manage token usage across your application:

```typescript
// Count tokens before sending to AI
const tokenCount = await $.Context.countTokens({
  text: largeDocument,
  model: 'gpt-5', // Different models use different tokenizers
})

// Enforce token budget
const context = await $.Context.create({
  content: documents,
  maxTokens: 8000,
  overflow: 'compress', // 'compress' | 'truncate' | 'error' | 'split'
})
```

### 3. Semantic Context Patterns

Use semantic patterns to structure context efficiently:

```typescript
// Create semantic context with typed structure
const context = await $.Context.create({
  $type: '$.Conversation.Context',
  system: systemPrompt,
  history: recentMessages,
  facts: extractedFacts,
  documents: relevantDocs,
  user: currentUser,
})

// Context automatically optimizes based on semantic importance
const optimized = await context.optimize({
  strategy: 'semantic',
  prioritize: ['system', 'facts', 'history'],
})
```

### 4. Multi-Turn Conversation Management

Efficiently manage long-running conversations:

```typescript
// Initialize conversation with sliding window
const conversation = await $.Conversation.create({
  systemPrompt: 'You are a helpful assistant',
  windowSize: 10, // Keep last 10 turns in full
  compressionThreshold: 20, // Compress when > 20 turns
  semanticMemory: true, // Extract and preserve key facts
})

// Add turns automatically manage context
await conversation.addTurn({
  role: 'user',
  content: 'What did we discuss about pricing?',
})

const response = await conversation.generate({
  model: 'claude-sonnet-4.5',
})

// Conversation automatically compresses old turns while preserving semantics
```

### 5. Workflow Context Passing

Pass context seamlessly between autonomous agents:

```typescript
// Create workflow context
const workflowContext = await $.WorkflowContext.create({
  $type: '$.CustomerSupport.Context',
  customer: customerData,
  issue: issueDescription,
  history: supportHistory,
  metadata: { priority: 'high', category: 'billing' },
})

// Agent 1: Triage
await $.send({
  event: '$.SupportTicket.created',
  context: workflowContext,
  to: '$.TriageAgent',
})

// Agent 2: Resolution (receives compressed context)
$.on('$.SupportTicket.triaged', async ({ context }) => {
  // Context automatically compressed to essentials
  const resolution = await $.ai.generate({
    model: 'gpt-5',
    context: context.compressed,
    prompt: 'Resolve the customer issue',
  })
})
```

### 6. Context Window Strategies

Multiple strategies for different use cases:

#### Sliding Window

Keep recent messages, discard old ones:

```typescript
const context = await $.Context.compress({
  messages: history,
  strategy: 'sliding',
  windowSize: 10,
})
```

#### Semantic Summarization

Compress old messages into semantic summaries:

```typescript
const context = await $.Context.compress({
  messages: history,
  strategy: 'summarize',
  summaryTokens: 500,
  preserveRecent: 5,
})
```

#### Hierarchical Context

Maintain multi-level context hierarchy:

```typescript
const context = await $.Context.compress({
  messages: history,
  strategy: 'hierarchical',
  levels: {
    recent: 5, // Last 5 messages: full detail
    medium: 10, // Next 10: moderate detail
    old: 'summary', // Older: summarized
  },
})
```

#### Semantic Memory

Extract and preserve key facts:

```typescript
const context = await $.Context.compress({
  messages: history,
  strategy: 'semantic',
  extractFacts: true,
  factTypes: ['$.Customer.Name', '$.Order.ID', '$.Product.Name'],
})
```

## Quick Start

### Installation

```bash
npm install sdk.do
# or
pnpm add sdk.do
```

### Basic Usage

```typescript
import { $ } from 'sdk.do'

// 1. Create context with token limit
const context = await $.Context.create({
  content: [
    { role: 'system', content: 'You are a helpful assistant' },
    { role: 'user', content: 'Tell me about context management' },
  ],
  maxTokens: 4000,
})

// 2. Count tokens
const tokens = await $.Context.countTokens({
  text: context.content,
  model: 'gpt-5',
})

console.log(`Context uses ${tokens} tokens`)

// 3. Compress if needed
if (tokens > 3000) {
  const compressed = await $.Context.compress({
    messages: context.content,
    targetTokens: 2000,
    strategy: 'semantic',
  })

  console.log(`Compressed from ${tokens} to ${compressed.tokens} tokens`)
}

// 4. Use in AI generation
const response = await $.ai.generate({
  model: 'gpt-5',
  context: compressed.messages,
  prompt: 'Continue the conversation',
})
```

### Multi-Turn Conversation

```typescript
import { $ } from 'sdk.do'

// Initialize conversation
const conv = await $.Conversation.create({
  systemPrompt: 'You are a helpful assistant',
  windowSize: 10,
  compressionThreshold: 15,
  semanticMemory: true,
})

// Add turns
await conv.addTurn({
  role: 'user',
  content: 'What is context management?',
})

const response1 = await conv.generate({
  model: 'gpt-5',
})

await conv.addTurn({
  role: 'assistant',
  content: response1.content,
})

// Continue conversation
await conv.addTurn({
  role: 'user',
  content: 'How does compression work?',
})

const response2 = await conv.generate({
  model: 'gpt-5',
})

// Context automatically managed
console.log(`Total turns: ${conv.turns.length}`)
console.log(`Context tokens: ${conv.currentTokens}`)
console.log(`Compressed turns: ${conv.compressedTurns}`)
```

## Use Cases

### 1. Long-Running Customer Support Chats

Maintain context across extended support conversations without hitting token limits:

```typescript
const supportConv = await $.Conversation.create({
  $type: '$.CustomerSupport.Conversation',
  systemPrompt: 'You are a customer support agent',
  windowSize: 8,
  compressionThreshold: 12,
  semanticMemory: true,
  factTypes: ['$.Customer.Name', '$.Order.ID', '$.Issue.Type', '$.Resolution.Status'],
})

// Conversation can run for 50+ turns while staying under token limits
// Key facts (customer name, order ID, issue) preserved in semantic memory
```

### 2. Document Analysis with Large Context

Analyze documents larger than model context windows:

```typescript
const largeDoc = await $.db.get('Document', docId)

// Split and analyze with overlapping context
const analysis = await $.Context.analyzeDocument({
  document: largeDoc.content,
  chunkSize: 8000,
  overlap: 500,
  strategy: 'semantic',
  extractFacts: true,
})

// Get comprehensive analysis without losing context
console.log(analysis.summary)
console.log(analysis.facts)
console.log(analysis.insights)
```

### 3. Multi-Agent Workflows

Pass context efficiently between autonomous agents:

```typescript
// Agent 1: Research
const researchContext = await $.WorkflowContext.create({
  $type: '$.Research.Context',
  topic: 'AI context management',
  sources: await $.api.search({ query: 'context management' }),
  requirements: ['comprehensive', 'technical', 'recent'],
})

await $.send({
  event: '$.Research.completed',
  context: researchContext.compress({ targetTokens: 2000 }),
})

// Agent 2: Writing (receives compressed context)
$.on('$.Research.completed', async ({ context }) => {
  const article = await $.ai.generate({
    model: 'gpt-5',
    context: context.expand(), // Expand essential context
    prompt: 'Write a comprehensive article',
  })
})
```

### 4. Semantic Search with Context

Combine semantic search with context management:

```typescript
// Search for relevant documents
const results = await $.db.search({
  query: userQuery,
  type: 'Document',
  limit: 20,
})

// Create context from top results with token budget
const context = await $.Context.fromSearchResults({
  results: results,
  maxTokens: 6000,
  strategy: 'semantic',
  relevanceThreshold: 0.7,
})

// Generate response with optimal context
const response = await $.ai.generate({
  model: 'claude-sonnet-4.5',
  context: context.messages,
  prompt: userQuery,
})
```

### 5. Code Review with Full Repository Context

Provide AI with repository context for code review:

```typescript
// Load repository structure
const repo = await $.db.get('Repository', repoId)

// Create hierarchical context
const context = await $.Context.createHierarchical({
  levels: [
    {
      name: 'architecture',
      content: repo.readme + repo.architecture,
      priority: 'high',
    },
    {
      name: 'related_files',
      content: await $.Context.getRelatedFiles(changedFiles),
      priority: 'medium',
    },
    {
      name: 'full_files',
      content: changedFiles,
      priority: 'critical',
    },
  ],
  maxTokens: 32000,
})

// Review with full context
const review = await $.ai.generate({
  model: 'gpt-5',
  context: context.optimized,
  prompt: 'Review these code changes',
})
```

## Architecture

### Context Compression Strategies

`context.do` implements multiple compression strategies optimized for different scenarios:

#### 1. Sliding Window

- **Best for**: Real-time conversations, chat applications
- **Method**: Keep N most recent messages, discard older ones
- **Pros**: Simple, fast, predictable token usage
- **Cons**: Loses historical context completely

#### 2. Semantic Summarization

- **Best for**: Long documents, detailed conversations
- **Method**: AI-powered summarization of old content
- **Pros**: Preserves meaning, good compression ratio
- **Cons**: Requires AI call, slightly slower

#### 3. Hierarchical Context

- **Best for**: Complex workflows, multi-topic conversations
- **Method**: Multiple compression levels (recent/medium/old)
- **Pros**: Balances detail and compression
- **Cons**: More complex, requires tuning

#### 4. Semantic Memory

- **Best for**: Fact-heavy conversations, customer support
- **Method**: Extract structured facts, discard prose
- **Pros**: Perfect recall of key information
- **Cons**: Requires schema definition

### Token Counting

Accurate token counting for different models:

```typescript
// GPT-5 uses cl100k_base tokenizer
const gptTokens = await $.Context.countTokens({
  text: content,
  model: 'gpt-5',
})

// Claude uses different tokenizer
const claudeTokens = await $.Context.countTokens({
  text: content,
  model: 'claude-sonnet-4.5',
})

// Results differ by ~10-15%
console.log(`GPT-5: ${gptTokens}, Claude: ${claudeTokens}`)
```

### Context Window Limits

Different models have different context windows:

| Model             | Context Window | Recommended Budget |
| ----------------- | -------------- | ------------------ |
| GPT-5             | 128K tokens    | 100K tokens        |
| GPT-5 Mini        | 128K tokens    | 100K tokens        |
| Claude Sonnet 4.5 | 200K tokens    | 180K tokens        |
| GPT-5 Nano        | 32K tokens     | 28K tokens         |

**Recommended Budget**: Leave 20% headroom for response generation

## Performance

### Token Reduction

Real-world compression results:

```typescript
// 50-turn customer support conversation
// Original: 15,234 tokens
// After semantic compression: 4,123 tokens (73% reduction)

// 100-page document analysis
// Original: 89,456 tokens (exceeds limit)
// After hierarchical compression: 32,000 tokens (64% reduction)

// Multi-agent workflow context
// Original: 8,734 tokens
// After workflow compression: 1,456 tokens (83% reduction)
```

### Latency

Context operations are fast:

```typescript
// Token counting: ~50ms for 10K tokens
// Sliding window: ~10ms (in-memory)
// Semantic summarization: ~1-2s (requires AI call)
// Hierarchical compression: ~100ms
// Fact extraction: ~500ms
```

### Cost Optimization

Context management reduces AI costs significantly:

```typescript
// Without context management:
// 50-turn conversation: 15K context tokens × 50 turns = 750K tokens
// Cost: $0.15 (at $0.20/1M tokens for GPT-5)

// With context management:
// 50-turn conversation: 4K context tokens × 50 turns = 200K tokens
// Cost: $0.04 (73% cost reduction)

// Annual savings for 10K conversations: $1,100
```

## Best Practices

### 1. Choose the Right Strategy

```typescript
// Real-time chat: sliding window
const chatContext = await $.Context.compress({
  strategy: 'sliding',
  windowSize: 10,
})

// Customer support: semantic memory
const supportContext = await $.Context.compress({
  strategy: 'semantic',
  extractFacts: true,
})

// Document analysis: hierarchical
const docContext = await $.Context.compress({
  strategy: 'hierarchical',
  levels: { recent: 3, medium: 7, old: 'summary' },
})
```

### 2. Set Appropriate Token Budgets

```typescript
// Leave headroom for response
const context = await $.Context.create({
  maxTokens: modelLimit * 0.8, // 80% of model limit
  overflow: 'compress',
})
```

### 3. Monitor Token Usage

```typescript
// Track tokens in production
const context = await $.Context.create({
  content: messages,
  onTokenCountChange: (tokens) => {
    $.metrics.gauge('context.tokens', tokens)

    if (tokens > threshold) {
      $.metrics.increment('context.overflow')
    }
  },
})
```

### 4. Preserve Critical Information

```typescript
// Always preserve system prompt and recent messages
const context = await $.Context.compress({
  messages: history,
  preserveFirst: 1, // System prompt
  preserveRecent: 3, // Last 3 turns
  compressMiddle: true,
})
```

### 5. Cache Compressed Context

```typescript
// Cache expensive compressions
const cacheKey = `context:${conversationId}:${messageCount}`

let context = await $.cache.get(cacheKey)

if (!context) {
  context = await $.Context.compress({
    messages: history,
    strategy: 'semantic',
  })

  await $.cache.set(cacheKey, context, { ttl: 3600 })
}
```

## API Overview

### Core Functions

```typescript
// Create context
$.Context.create(options)

// Compress context
$.Context.compress(options)

// Count tokens
$.Context.countTokens(options)

// Optimize context
$.Context.optimize(options)

// Extract facts
$.Context.extractFacts(options)
```

### Conversation Management

```typescript
// Create conversation
$.Conversation.create(options)

// Add turn
conversation.addTurn(message)

// Generate response
conversation.generate(options)

// Get context summary
conversation.getSummary()
```

### Workflow Context

```typescript
// Create workflow context
$.WorkflowContext.create(options)

// Compress for transfer
workflowContext.compress(options)

// Expand at destination
workflowContext.expand()
```

## Examples

### Example 1: Basic Context Compression

```typescript
const compressed = await $.Context.compress({
  messages: longConversation,
  targetTokens: 4000,
  strategy: 'semantic',
})
```

### Example 2: Multi-Turn Conversation

```typescript
const conv = await $.Conversation.create({
  windowSize: 10,
  compressionThreshold: 15,
})

await conv.addTurn({ role: 'user', content: 'Hello' })
const response = await conv.generate({ model: 'gpt-5' })
```

### Example 3: Document Analysis

```typescript
const analysis = await $.Context.analyzeDocument({
  document: largeDocument,
  chunkSize: 8000,
  strategy: 'semantic',
})
```

## Resources

- [Getting Started Guide](./docs/getting-started.mdx) - Setup and basic usage
- [Architecture](./docs/architecture.mdx) - Technical details and strategies
- [Best Practices](./docs/best-practices.mdx) - Optimization tips and patterns
- [Troubleshooting](./docs/troubleshooting.mdx) - Common issues and solutions
- [API Reference](./api/reference.mdx) - Complete API documentation
- [Examples](./examples/) - Real-world usage examples

## Related Platforms

- [llm.do](https://llm.do) - AI model integration and generation
- [workflows.do](https://workflows.do) - Autonomous workflow orchestration
- [agents.do](https://agents.do) - Multi-agent systems
- [embeddings.do](https://embeddings.do) - Semantic search and embeddings

## Support

- **Documentation**: https://context.do
- **Issues**: https://github.com/dot-do/platform/issues
- **Discord**: https://discord.gg/dotdo

## License

- Documentation: [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/)
- Code Examples: [MIT](https://opensource.org/licenses/MIT)

---

Built with [sdk.do](https://sdk.do) - The semantic SDK for Business-as-Code
